{
  "name": "robot-directives",
  "description": "Parse robot directives within HTML meta and/or HTTP headers.",
  "version": "0.5.0-alpha",
  "license": "MIT",
  "author": "Steven Vachon <contact@svachon.com> (https://svachon.com)",
  "repository": "github:stevenvachon/robot-directives",
  "main": "lib",
  "browser": "lib-es5",
  "dependencies": {
    "deep-freeze-node": "^1.1.3",
    "isbot": "^3.0.19",
    "useragent": "^2.3.0"
  },
  "devDependencies": {
    "@babel/cli": "^7.6.0",
    "@babel/core": "^7.6.0",
    "@babel/preset-env": "^7.6.0",
    "chai": "^4.2.0",
    "coveralls": "^3.0.6",
    "mocha": "^6.2.0",
    "nyc": "^14.1.1"
  },
  "engines": {
    "node": ">= 8"
  },
  "scripts": {
    "ci": "npm test && nyc report --reporter=text-lcov | coveralls",
    "posttest": "nyc report --reporter=text-summary --reporter=html",
    "prepublishOnly": "npm test && babel lib/ --out-dir=lib-es5/ --presets=@babel/env --source-maps",
    "test": "nyc --silent mocha test.js --bail --check-leaks"
  },
  "files": [
    "lib",
    "lib-es5"
  ],
  "keywords": [
    "crawlers",
    "header",
    "html",
    "http",
    "meta",
    "metadata",
    "nofollow",
    "noindex",
    "robots",
    "robots.txt",
    "seo",
    "spiders"
  ]
}
